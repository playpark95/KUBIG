{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JWG30YuzV7I"
   },
   "source": [
    "# **LSTM + 전처리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAnbMhCw0icL"
   },
   "source": [
    "# 라이브러리 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WpRuMNjw0f1R"
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import metrics, preprocessing, pipeline, model_selection, naive_bayes\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "import time\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling1D, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RwDd-gSm0_Bo"
   },
   "outputs": [],
   "source": [
    "#import datasets\n",
    "\n",
    "train = pd.read_csv('G:/From 2021/2021/2021-Pre/학회_시즌2/딥러닝 분반/소대회/train.csv', encoding = \"cp949\")\n",
    "test = pd.read_csv('G:/From 2021/2021/2021-Pre/학회_시즌2/딥러닝 분반/소대회/test_x.csv')\n",
    "sample = pd.read_csv('G:/From 2021/2021/2021-Pre/학회_시즌2/딥러닝 분반/소대회/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "PUbHPsy1B9Wy",
    "outputId": "ee2373e3-218a-40f3-9b45-936d233cff1e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  0  1  2  3  4\n",
       "0      0  0  0  0  0  0\n",
       "1      1  0  0  0  0  0\n",
       "2      2  0  0  0  0  0\n",
       "3      3  0  0  0  0  0\n",
       "4      4  0  0  0  0  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if samples are properly imported\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqRkrekDDEzk",
    "outputId": "5e77e5a5-5966-47c0-f417-d326abfca8b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    15063\n",
       "0    13235\n",
       "2    11554\n",
       "4     7805\n",
       "1     7222\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking a distribution\n",
    "train['author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6JFkFX9B-lg",
    "outputId": "7dfedf35-684c-41ca-e2b6-10eafd2b4471"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19617, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSCThoudYeA_"
   },
   "source": [
    "# Feature Engineering 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KS_m4uZGb0mU"
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z7beBnZOBGAN",
    "outputId": "d1bab974-74d1-41a7-93f9-72042e5940ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\playp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RGBm7dP9AZvI"
   },
   "outputs": [],
   "source": [
    "## Number of words in the text \n",
    "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\n",
    "test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "eng_stopwords = nltk.corpus.stopwords.words('english')\n",
    "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cY8I5YiwBO7r",
    "outputId": "0f703a75-849d-445e-99ea-85828ff1c955"
   },
   "outputs": [],
   "source": [
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MS7960P9CnwZ"
   },
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def clean_text(text):\n",
    "    return re.sub('[^a-zA-Z]', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dDXY5ByCCrPT"
   },
   "outputs": [],
   "source": [
    "train['text_cleaned'] = train['text'].apply(lambda x: clean_text(x))\n",
    "test['text_cleaned'] = test['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZVIdCSLJDrTe"
   },
   "outputs": [],
   "source": [
    "#문장부호 등 확인\n",
    "\n",
    "def extract_features(df):\n",
    "    df['len'] = df['text'].apply(lambda x: len(x))\n",
    "    df['n_words'] = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "    df['n_.'] = df['text'].str.count('\\.')\n",
    "    df['n_...'] = df['text'].str.count('\\...')\n",
    "    df['n_,'] = df['text'].str.count('\\,')\n",
    "    df['n_:'] = df['text'].str.count('\\:')\n",
    "    df['n_;'] = df['text'].str.count('\\;')\n",
    "    df['n_-'] = df['text'].str.count('\\-')\n",
    "    df['n_?'] = df['text'].str.count('\\?')\n",
    "    df['n_!'] = df['text'].str.count('\\!')\n",
    "    df['n_\\''] = df['text'].str.count('\\'')\n",
    "    df['n_\"'] = df['text'].str.count('\\\"')\n",
    "    df[\"n_“\"] = df['text'].str.count('“')\n",
    "    df[\"n_”\"] = df['text'].str.count('”')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oX_aj6wLEM4t",
    "outputId": "baad834a-a60d-432f-82fe-81c6856b5a45",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n",
      "Processing test...\n"
     ]
    }
   ],
   "source": [
    "print('Processing train...')\n",
    "extract_features(train)\n",
    "print('Processing test...')\n",
    "extract_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging similar features into a feature\n",
    "def selecting_features(df):\n",
    "    df[\"n_quotes\"]=df['n_\"']+df['n_“']+df['n_”']\n",
    "    df.drop(['n_\"'], axis=1, inplace=True)\n",
    "    df.drop(['n_“'], axis=1, inplace=True)\n",
    "    df.drop(['n_”'], axis=1, inplace=True)\n",
    "    df.drop(['num_words_title'], axis=1, inplace=True)\n",
    "\n",
    "selecting_features(train)\n",
    "selecting_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nums = train.copy()\n",
    "test_nums = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nums_only(df):\n",
    "    df.drop(['text'],axis=1,inplace=True)\n",
    "    df.drop(['text_cleaned'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_only(train_nums)\n",
    "nums_only(test_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>len</th>\n",
       "      <th>...</th>\n",
       "      <th>n_.</th>\n",
       "      <th>n_...</th>\n",
       "      <th>n_,</th>\n",
       "      <th>n_:</th>\n",
       "      <th>n_;</th>\n",
       "      <th>n_-</th>\n",
       "      <th>n_?</th>\n",
       "      <th>n_!</th>\n",
       "      <th>n_'</th>\n",
       "      <th>n_quotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>39</td>\n",
       "      <td>240</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.239130</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>50</td>\n",
       "      <td>320</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4.614035</td>\n",
       "      <td>320</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "      <td>319</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>4.517241</td>\n",
       "      <td>319</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>228</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4.871795</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  author  num_words  num_unique_words  num_chars  num_stopwords  \\\n",
       "0      0       3         46                39        240             25   \n",
       "1      1       2          7                 7         38              2   \n",
       "2      2       1         57                50        320             26   \n",
       "3      3       4         58                49        319             26   \n",
       "4      4       3         39                36        228             16   \n",
       "\n",
       "   num_punctuations  num_words_upper  mean_word_len  len  ...  n_.  n_...  \\\n",
       "0                 8                0       4.239130  240  ...    3      2   \n",
       "1                 2                1       4.571429   38  ...    0      0   \n",
       "2                 9                0       4.614035  320  ...    2      2   \n",
       "3                18                0       4.517241  319  ...    6      5   \n",
       "4                13                0       4.871795  228  ...    6      4   \n",
       "\n",
       "   n_,  n_:  n_;  n_-  n_?  n_!  n_'  n_quotes  \n",
       "0    4    0    1    0    0    0    0         0  \n",
       "1    1    0    0    0    1    0    0         2  \n",
       "2    6    1    0    0    0    0    0         0  \n",
       "3    9    0    2    0    0    0    1         2  \n",
       "4    4    0    1    0    0    2    0         4  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting Features(only quants) for later use\n",
    "\n",
    "train_nums.to_csv('G:/From 2021/2021/2021-Pre/학회_시즌2/딥러닝 분반/소대회/train_nums.csv',index=False)\n",
    "test_nums.to_csv('G:/From 2021/2021/2021-Pre/학회_시즌2/딥러닝 분반/소대회/test_nums.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 2차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6_U1EV3FPLo",
    "outputId": "e5575810-266f-498c-84a8-672bba8ba149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (54879, 2137863)\n",
      "Tfidf_test: (19617, 1777117)\n"
     ]
    }
   ],
   "source": [
    "## add tfidf and svd \n",
    "# max_df option 어떻게 결정해야 할지 고민 !\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), min_df =0,max_df=0.9,lowercase=False, use_idf=True)\n",
    "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.fit_transform(test['text'].values.tolist())\n",
    "print('Tfidf_train:',train_tfidf.shape)\n",
    "print('Tfidf_test:',test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC8dsmmJB4KD"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UmHNVaKEviV",
    "outputId": "cb18f96d-f733-4fc9-a2b7-aa9586227c12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import keras done\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, GRU, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "print('import keras done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rxaYkEVhE5g-"
   },
   "outputs": [],
   "source": [
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tcMpDBKRKCnn"
   },
   "outputs": [],
   "source": [
    "# 여기서부터 1-D CNN\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model, to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1O-K0PVxI0GF",
    "outputId": "9260fdff-8ea6-4135-c9fb-785f395eedf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879,) (19617,) (54879,)\n"
     ]
    }
   ],
   "source": [
    "#Check if it is OK.\n",
    "\n",
    "X_train = train['text'].values\n",
    "X_test = test['text'].values\n",
    "y = train['author'].values\n",
    "print(X_train.shape, X_test.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j55UO4QBI3ZF",
    "outputId": "00cad4e9-4b40-47a2-effc-289b5e32bb36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['He was almost choking. There was so much, so much he wanted to say, but strange exclamations were all that came from his lips. The Pole gazed fixedly at him, at the bundle of notes in his hand; looked at odin, and was in evident perplexity.',\n",
       "       '“Your sister asked for it, I suppose?”',\n",
       "       ' She was engaged one day as she walked, in perusing Jane’s last letter, and dwelling on some passages which proved that Jane had not written in spirits, when, instead of being again surprised by Mr. odin, she saw on looking up that odin was meeting her. Putting away the letter immediately and forcing a smile, she said:'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "A9BREyUpItO0"
   },
   "outputs": [],
   "source": [
    "#Setting Hyperparameters\n",
    "\n",
    "vocab_size = 20000\n",
    "embedding_dim = 64\n",
    "max_length = 500\n",
    "padding_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dFDfeiPFIvjb"
   },
   "outputs": [],
   "source": [
    "#Tokenizing & Padding\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUdGM2kqJEAL",
    "outputId": "fd5d08b7-c7b2-48e9-a53e-db9e45b2b1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 500) (19617, 500)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n",
    "X_test = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 500 이하인 샘플의 비율: 100.0\n"
     ]
    }
   ],
   "source": [
    "#Check if it is all right\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if len(s) <= max_len:\n",
    "            cnt = cnt + 1 \n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
    "\n",
    "max_len = 500\n",
    "below_threshold_len(max_len,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import utilities\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 42 \n",
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 양방향 LSTM 모형 설정\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
    "import tensorflow as tf\n",
    "\n",
    "model2 = tf.keras.Sequential() \n",
    "model2.add(Embedding(vocab_size, 100))\n",
    "model2.add(Bidirectional(LSTM(100)))\n",
    "model2.add(Dense(5, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([x for x in train['author']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model for CV #1\n",
      "Epoch 1/7\n",
      "172/172 [==============================] - 1048s 6s/step - loss: 1.4139 - accuracy: 0.3854 - val_loss: 0.8660 - val_accuracy: 0.6663\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.86597, saving model to ./model16_4\\01-0.8660.hdf5\n",
      "Epoch 2/7\n",
      "172/172 [==============================] - 1164s 7s/step - loss: 0.7034 - accuracy: 0.7427 - val_loss: 0.6578 - val_accuracy: 0.7584\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.86597 to 0.65783, saving model to ./model16_4\\02-0.6578.hdf5\n",
      "Epoch 3/7\n",
      "172/172 [==============================] - 1190s 7s/step - loss: 0.4569 - accuracy: 0.8427 - val_loss: 0.6539 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65783 to 0.65388, saving model to ./model16_4\\03-0.6539.hdf5\n",
      "Epoch 4/7\n",
      "172/172 [==============================] - 1183s 7s/step - loss: 0.3386 - accuracy: 0.8850 - val_loss: 0.6584 - val_accuracy: 0.7723\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.65388\n",
      "Epoch 5/7\n",
      "172/172 [==============================] - 1212s 7s/step - loss: 0.2727 - accuracy: 0.9084 - val_loss: 0.7143 - val_accuracy: 0.7707\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.65388\n",
      "Epoch 6/7\n",
      "172/172 [==============================] - 1252s 7s/step - loss: 0.2388 - accuracy: 0.9182 - val_loss: 0.7371 - val_accuracy: 0.7691\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.65388\n",
      "Epoch 00006: early stopping\n",
      "training model for CV #2\n",
      "Epoch 1/7\n",
      "172/172 [==============================] - 1174s 7s/step - loss: 0.4204 - accuracy: 0.8532 - val_loss: 0.3704 - val_accuracy: 0.8706\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.65388 to 0.37036, saving model to ./model16_4\\01-0.3704.hdf5\n",
      "Epoch 2/7\n",
      "172/172 [==============================] - 1182s 7s/step - loss: 0.3289 - accuracy: 0.8871 - val_loss: 0.3966 - val_accuracy: 0.8546\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37036\n",
      "Epoch 3/7\n",
      "172/172 [==============================] - 1182s 7s/step - loss: 0.2747 - accuracy: 0.9062 - val_loss: 0.5394 - val_accuracy: 0.8077\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.37036\n",
      "Epoch 4/7\n",
      "172/172 [==============================] - 1176s 7s/step - loss: 0.2659 - accuracy: 0.9075 - val_loss: 0.4961 - val_accuracy: 0.8308\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.37036\n",
      "Epoch 00004: early stopping\n",
      "training model for CV #3\n",
      "Epoch 1/7\n",
      "172/172 [==============================] - 1171s 7s/step - loss: 0.3416 - accuracy: 0.8810 - val_loss: 0.3035 - val_accuracy: 0.8925\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.37036 to 0.30347, saving model to ./model16_4\\01-0.3035.hdf5\n",
      "Epoch 2/7\n",
      "172/172 [==============================] - 1169s 7s/step - loss: 0.2897 - accuracy: 0.9009 - val_loss: 0.3461 - val_accuracy: 0.8759\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.30347\n",
      "Epoch 3/7\n",
      "172/172 [==============================] - 1168s 7s/step - loss: 0.2584 - accuracy: 0.9111 - val_loss: 0.3954 - val_accuracy: 0.8559\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.30347\n",
      "Epoch 4/7\n",
      "172/172 [==============================] - 1166s 7s/step - loss: 0.2334 - accuracy: 0.9195 - val_loss: 0.4294 - val_accuracy: 0.8485\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.30347\n",
      "Epoch 00004: early stopping\n",
      "training model for CV #4\n",
      "Epoch 1/7\n",
      "172/172 [==============================] - 1170s 7s/step - loss: 0.3016 - accuracy: 0.8951 - val_loss: 0.2752 - val_accuracy: 0.9024\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.30347 to 0.27523, saving model to ./model16_4\\01-0.2752.hdf5\n",
      "Epoch 2/7\n",
      "172/172 [==============================] - 1168s 7s/step - loss: 0.2611 - accuracy: 0.9106 - val_loss: 0.3079 - val_accuracy: 0.8881\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.27523\n",
      "Epoch 3/7\n",
      "172/172 [==============================] - 1170s 7s/step - loss: 0.2277 - accuracy: 0.9219 - val_loss: 0.3413 - val_accuracy: 0.8789\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.27523\n",
      "Epoch 4/7\n",
      "172/172 [==============================] - 1167s 7s/step - loss: 0.2108 - accuracy: 0.9275 - val_loss: 0.4001 - val_accuracy: 0.8599\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.27523\n",
      "Epoch 00004: early stopping\n",
      "training model for CV #5\n",
      "Epoch 1/7\n",
      "172/172 [==============================] - 1162s 7s/step - loss: 0.2772 - accuracy: 0.9022 - val_loss: 0.2528 - val_accuracy: 0.9155\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27523 to 0.25281, saving model to ./model16_4\\01-0.2528.hdf5\n",
      "Epoch 2/7\n",
      "172/172 [==============================] - 1160s 7s/step - loss: 0.2430 - accuracy: 0.9149 - val_loss: 0.2971 - val_accuracy: 0.8932\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.25281\n",
      "Epoch 3/7\n",
      "172/172 [==============================] - 1160s 7s/step - loss: 0.2168 - accuracy: 0.9237 - val_loss: 0.3263 - val_accuracy: 0.8820\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25281\n",
      "Epoch 4/7\n",
      "172/172 [==============================] - 1161s 7s/step - loss: 0.1933 - accuracy: 0.9330 - val_loss: 0.3843 - val_accuracy: 0.8655\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25281\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "##모형 돌릴 준비!\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "MODEL_SAVE_FOLDER_PATH = './model16_4/' \n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH): \n",
    "    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "model_path = MODEL_SAVE_FOLDER_PATH + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "##parameter 추가 세팅\n",
    "n_class = 5 \n",
    "embedding_dim = 100\n",
    "max_length = max_len \n",
    "\n",
    "##0벡터 및 Early Stopping 설정\n",
    "p_val = np.zeros((X_train.shape[0], n_class))\n",
    "p_tst = np.zeros((X_test.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(X_train, y_train), 1):\n",
    "    print(f'training model for CV #{i}') \n",
    "    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3, verbose=1,\n",
    "                       mode='min', baseline=None, restore_best_weights=True)\n",
    "    ###모형 적용 \n",
    "    clf2 = model2\n",
    "    clf2.fit(X_train[i_trn], \n",
    "            to_categorical(y_train[i_trn]), \n",
    "            validation_data=(X_train[i_val], to_categorical(y_train[i_val])), \n",
    "            epochs=7, batch_size=256, \n",
    "            callbacks=[es, cb_checkpoint])\n",
    "    p_val[i_val, :] = clf2.predict(X_train[i_val])\n",
    "    p_tst += clf2.predict(X_test) / n_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhQ5FvqRNnYF",
    "outputId": "e90a2d22-1286-495b-fb6b-a138f34cd044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss (CV):   0.3712\n"
     ]
    }
   ],
   "source": [
    "#print(f'Accuracy (CV): {accuracy_score(y, np.argmax(p_val, axis=1)) * 100:8.4f}%')\n",
    "print(f'Log Loss (CV): {log_loss(pd.get_dummies(y), p_val):8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4GpN6pG1tZx",
    "outputId": "f6c0bbcf-5eb6-401c-c922-2d62cb6055f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1005      \n",
      "=================================================================\n",
      "Total params: 2,161,805\n",
      "Trainable params: 2,161,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(clf2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "PnpA-saR2Brt",
    "outputId": "240c18c0-69af-44d9-81e5-0b1d114295fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19617, 6)\n"
     ]
    }
   ],
   "source": [
    "##Exporting the output\n",
    "\n",
    "sample_file = \"G:/From 2021/2021/2021-Pre/학회_시즌2/딥러닝 분반/소대회/sample_submission.csv\" \n",
    "sub = pd.read_csv(sample_file) \n",
    "print(sub.shape) \n",
    "\n",
    "sub[['0', '1', '2', '3', '4']] = p_tst \n",
    "sub.to_csv(\"G:/From 2021/2021/2021-Pre/학회_시즌2/딥러닝 분반/소대회/submission_LSTM.csv\", index=False) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "데이터분석대회_세린.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
